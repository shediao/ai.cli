#ifndef __AI_CLI_SRC_RESPONSE_H__
#define __AI_CLI_SRC_RESPONSE_H__
#include <functional>
#include <memory>
#include <nlohmann/json.hpp>
#include <vector>
using json = nlohmann::json;

namespace ai {

namespace openai {

struct ChatCompletionSnapshot {
  struct Choice {
    struct Message {
      struct ToolCall {
        struct Function {
          /**
           * The arguments to call the function with, as generated by the model
           * in JSON format. Note that the model does not always generate valid
           * JSON, and may hallucinate parameters not defined by your function
           * schema. Validate the arguments in your code before calling your
           * function.
           */
          std::string arguments_;

          /**
           * The name of the function to call.
           */
          std::string name_;
        };
        /**
         * The ID of the tool call.
         */
        std::string id_;

        Function function_;

        /**
         * The type of the tool.
         */
        std::string type{"function"};
      };
      /**
       * The contents of the chunk message.
       */
      std::optional<std::string> content{std::nullopt};
      std::optional<std::string> reasoning_content{std::nullopt};

      /**
       * The name and arguments of a function that should be called, as
       * generated by the model.
       */
      std::vector<ToolCall> tool_calls_;

      /**
       * The role of the author of this message.
       */
      std::string role_;
    };
    /**
     * A chat completion delta generated by streamed model responses.
     */
    Message message_;

    /**
     * The reason the model stopped generating tokens. This will be `stop` if
     * the model hit a natural stop point or a provided stop sequence, `length`
     * if the maximum number of tokens specified in the request was reached,
     * `content_filter` if content was omitted due to a flag from our content
     * filters, or `function_call` if the model called a function.
     */
    std::optional<std::string> finish_reason_{std::nullopt};

    /**
     * The index of the choice in the list of choices.
     */
    int index_;
  };
  /**
   * A unique identifier for the chat completion.
   */
  std::string id_;

  /**
   * A list of chat completion choices. Can be more than one if `n` is greater
   * than 1.
   */
  std::vector<Choice> choices_;

  /**
   * The Unix timestamp (in seconds) of when the chat completion was created.
   */
  unsigned long long created_;

  /**
   * The model to generate the completion.
   */
  std::string model_;

  // Note we do not include an "object" type on the snapshot,
  // because the object is not a valid "chat.completion" until finalized.
  // object: 'chat.completion';

  /**
   * This fingerprint represents the backend configuration that the model runs
   * with.
   *
   * Can be used in conjunction with the `seed` request parameter to understand
   * when backend changes have been made that might impact determinism.
   */
  std::optional<std::string> system_fingerprint_{std::nullopt};
};

class Response {
 public:
  struct Function {
    /**
     * The arguments to call the function with, as generated by the model
     * in JSON format. Note that the model does not always generate valid
     * JSON, and may hallucinate parameters not defined by your function
     * schema. Validate the arguments in your code before calling your
     * function.
     */
    std::string arguments;

    /**
     * The name of the function to call.
     */
    std::string name;
  };
  struct ToolCall {
    // The ID of the tool call.
    std::string id;
    // The type of the tool.
    std::string type{"function"};
    Function function;
  };
  struct Message {
    // The role of the author of this message.
    std::string role;
    // The contents of the chunk message.
    std::string content;
    std::string reasoning_content;
    // The name and arguments of a function that should be called, as generated
    // by the model.
    std::vector<ToolCall> tool_calls;
    json tool_calls_json() const;
  };
  struct Choice {
    int index{-1};
    Message message;
    std::string finish_reason;
  };
  struct Usage {
    int prompt_tokens{0};
    int completion_tokens{0};
    int total_tokens{0};
  };
  Response(Response&&) = default;
  Response& operator=(Response&&) = default;
  Response(Response const&) = default;
  Response& operator=(Response const&) = default;
  ~Response() = default;
  static Response from_string(std::string const& response_data);
  static Response from_json(json const& response_json);
  static Response from_sse_json(json const& sse_json);
  inline std::vector<Choice> const& choices() const { return choices_; }
  inline Usage const& usage() const { return usage_; }
  inline std::string const& id() { return id_; }
  inline std::string const& model() { return model_; }

  void add_to_history(json& history);

 private:
  Response() = default;
  std::string id_;
  std::string model_;
  std::vector<Choice> choices_;
  Usage usage_;
};

class StreamResponse {
 public:
  StreamResponse(std::ostream& out);

  static size_t parse(const char* ptr, size_t size, size_t nmemb,
                      StreamResponse*);
  Response toResponse();
  std::string_view raw_string();

 private:
  json all_json_data_{json::array()};
  void parse_impl();
  std::vector<char> response_data_;
  std::size_t parse_index_{0};
  std::reference_wrapper<std::ostream> out_;
};
}  // namespace openai

namespace gemini {}

}  // namespace ai

#endif  // __AI_CLI_SRC_RESPONSE_H__
